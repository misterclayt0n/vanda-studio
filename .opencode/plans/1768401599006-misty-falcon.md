# AI Module Refactoring: Vercel AI SDK + Effect

## Overview
Refactor `src/convex/ai/llm.ts` from raw fetch-based OpenRouter API calls to a typed Effect-based architecture using **Vercel AI SDK** (`ai` package with `@openrouter/ai-sdk-provider`) for AI operations and **Effect** for error handling and composition.

## Current Problems
- No typed errors (just `throw new Error("string")`)
- No retry logic
- Manual JSON parsing with hacky repair functions (`tryRepairJSON`)
- Direct `process.env` access
- Mixed language error messages (EN/PT-BR)
- `callVisionLLM` is unused but we keep for vision analysis

## Target Architecture

### File Structure
```
src/convex/ai/
├── llm/
│   ├── index.ts              # Public API exports
│   ├── errors.ts             # Custom domain errors (PT-BR user messages)
│   ├── config.ts             # Effect Config for env vars
│   ├── models.ts             # Model constants
│   ├── services/
│   │   ├── TextGeneration.ts # Text/caption generation via Vercel AI SDK
│   │   └── ImageGeneration.ts # Image generation (custom, Gemini multimodal)
│   └── runtime.ts            # Convex action runtime adapter
├── prompts.ts                # Keep unchanged
├── enhancedPostGeneration.ts # Update to use new module
└── regenerate.ts             # Update to use new module
```

## Dependencies to Install
```bash
bun add ai @openrouter/ai-sdk-provider zod
```

## Implementation Tasks

### 1. Install Dependencies
- `ai` - Vercel AI SDK core
- `@openrouter/ai-sdk-provider` - OpenRouter provider for Vercel AI SDK
- `zod` - Schema validation (used by AI SDK's structured output)

### 2. Create Error Types (`llm/errors.ts`)
Custom Error classes with PT-BR user-facing messages:

```typescript
import { Data } from "effect";

export class ConfigurationError extends Data.TaggedError("ConfigurationError")<{
  readonly key: string;
}> {
  get userMessage(): string {
    return "Erro de configuração do servidor. Por favor, tente novamente mais tarde.";
  }
}

export class RateLimitError extends Data.TaggedError("RateLimitError")<{
  readonly retryAfter?: number;
}> {
  get userMessage(): string {
    return "Muitas requisições. Por favor, aguarde alguns segundos e tente novamente.";
  }
}

export class MalformedResponseError extends Data.TaggedError("MalformedResponseError")<{
  readonly rawResponse: string;
}> {
  get userMessage(): string {
    return "Resposta inválida do modelo de IA. Por favor, tente novamente.";
  }
}

export class ImageGenerationError extends Data.TaggedError("ImageGenerationError")<{
  readonly reason: string;
}> {
  get userMessage(): string {
    return "Falha ao gerar imagem. Por favor, tente novamente.";
  }
}

export class NetworkError extends Data.TaggedError("NetworkError")<{
  readonly cause: unknown;
}> {
  get userMessage(): string {
    return "Erro de conexão. Verifique sua internet e tente novamente.";
  }
}

export type AiError =
  | ConfigurationError
  | RateLimitError
  | MalformedResponseError
  | ImageGenerationError
  | NetworkError;
```

### 3. Create Config (`llm/config.ts`)
```typescript
import { Config, Effect } from "effect";
import { ConfigurationError } from "./errors";

export const OpenRouterApiKey = Config.string("OPENROUTER_API_KEY").pipe(
  Config.withDefault(undefined),
  Effect.flatMap((key) =>
    key ? Effect.succeed(key) : Effect.fail(new ConfigurationError({ key: "OPENROUTER_API_KEY" }))
  )
);

export const SiteUrl = Config.string("SITE_URL").pipe(
  Config.withDefault("https://vanda.studio")
);
```

### 4. Create Model Constants (`llm/models.ts`)
Move existing MODELS constant, keep unchanged.

### 5. Create TextGeneration Service (`llm/services/TextGeneration.ts`)
Uses Vercel AI SDK with Effect wrapper:

```typescript
import { Effect, Context } from "effect";
import { createOpenRouter } from "@openrouter/ai-sdk-provider";
import { generateText, Output } from "ai";
import { z } from "zod";
import { MODELS } from "../models";
import { MalformedResponseError, RateLimitError, NetworkError } from "../errors";

// Service interface
export class TextGeneration extends Context.Tag("TextGeneration")<
  TextGeneration,
  {
    readonly generateCaption: (params: {
      systemPrompt: string;
      userPrompt: string;
      temperature?: number;
    }) => Effect.Effect<CaptionResponse, AiError>;
    
    readonly generateStructured: <T>(params: {
      systemPrompt: string;
      userPrompt: string;
      schema: z.ZodSchema<T>;
      temperature?: number;
    }) => Effect.Effect<T, AiError>;
  }
>() {}

// Implementation that wraps Vercel AI SDK calls in Effect
export const TextGenerationLive = Effect.gen(function* () {
  const apiKey = yield* OpenRouterApiKey;
  const siteUrl = yield* SiteUrl;
  
  const openrouter = createOpenRouter({
    apiKey,
    headers: {
      "HTTP-Referer": siteUrl,
      "X-Title": "Vanda Studio",
    },
  });

  return {
    generateCaption: (params) =>
      Effect.tryPromise({
        try: async () => {
          const { output } = await generateText({
            model: openrouter.chat(MODELS.GPT_4_1),
            system: params.systemPrompt,
            prompt: params.userPrompt,
            temperature: params.temperature ?? 0.7,
            output: Output.object({
              schema: z.object({
                caption: z.string(),
                hashtags: z.array(z.string()),
                reasoning: z.string().optional(),
              }),
            }),
          });
          return output;
        },
        catch: (error) => mapAiSdkError(error),
      }),
      
    generateStructured: (params) =>
      Effect.tryPromise({
        try: async () => {
          const { output } = await generateText({
            model: openrouter.chat(MODELS.GPT_4_1),
            system: params.systemPrompt,
            prompt: params.userPrompt,
            temperature: params.temperature ?? 0.7,
            output: Output.object({ schema: params.schema }),
          });
          return output;
        },
        catch: (error) => mapAiSdkError(error),
      }),
  };
});
```

### 6. Create ImageGeneration Service (`llm/services/ImageGeneration.ts`)
Custom implementation for Gemini 3 Pro Image (not supported by AI SDK structured output):
- Keep existing `generateImage` logic but wrap in Effect
- Keep `fetchImageAsBase64` helper for reference images
- Map errors to typed domain errors

### 7. Create Runtime Adapter (`llm/runtime.ts`)
```typescript
import { Effect, Layer, ManagedRuntime } from "effect";
import { TextGenerationLive } from "./services/TextGeneration";
import { ImageGenerationLive } from "./services/ImageGeneration";
import type { AiError } from "./errors";

// Combined layer
const AiLive = Layer.mergeAll(TextGenerationLive, ImageGenerationLive);

// Runtime for Convex actions
const runtime = ManagedRuntime.make(AiLive);

// Run Effect in Convex action context
export async function runAiEffect<A>(
  effect: Effect.Effect<A, AiError, TextGeneration | ImageGeneration>
): Promise<A> {
  return runtime.runPromise(effect);
}

// Extract user-friendly error message
export function getErrorMessage(error: AiError): string {
  return error.userMessage;
}
```

### 8. Create Public API (`llm/index.ts`)
Export all services, errors, types, and runtime.

### 9. Migrate `enhancedPostGeneration.ts`
Update to use new Effect-based services:
```typescript
// Before
const response = await callLLM(messages, { jsonMode: true });
const parsed = parseJSONResponse(response.content);

// After  
const result = await runAiEffect(
  Effect.gen(function* () {
    const textGen = yield* TextGeneration;
    return yield* textGen.generateCaption({
      systemPrompt: CAPTION_SYSTEM_PROMPT,
      userPrompt: buildUserPrompt(input),
    });
  })
);
```

### 10. Migrate `regenerate.ts`
Update both `regenerateCaption` and `regenerateImage` actions.

### 11. Fix TypeScript Errors
Address `exactOptionalPropertyTypes` errors in:
- `src/convex/generatedPosts.ts`
- `src/convex/ai/regenerate.ts`
- `src/convex/ai/enhancedPostGeneration.ts`
- `src/convex/users.ts`

Use explicit `undefined` or conditional spreading patterns.

### 12. Delete Old Implementation
Remove `src/convex/ai/llm.ts` after successful migration.

## Critical Files to Modify
- `src/convex/ai/llm.ts` → DELETE after migration
- `src/convex/ai/enhancedPostGeneration.ts` → Update to use new module
- `src/convex/ai/regenerate.ts` → Update to use new module
- `src/convex/generatedPosts.ts` → Fix TS errors
- `src/convex/users.ts` → Fix TS errors
- `package.json` → Add `ai`, `@openrouter/ai-sdk-provider`, `zod`

## Key Vercel AI SDK Features Used
1. **Structured Output** - `Output.object({ schema })` with Zod for typed JSON responses
2. **Provider Abstraction** - `@openrouter/ai-sdk-provider` for OpenRouter
3. **Built-in error handling** - AI SDK provides typed errors we can map to our domain errors

## Verification
1. `bun run check` - No TypeScript errors
2. Test caption generation via UI (create new post)
3. Test image generation via UI
4. Test regeneration (caption + image separately)
5. Test error scenarios:
   - Invalid API key → ConfigurationError → PT-BR message
   - Rate limit (if triggerable) → RateLimitError → PT-BR message
   - Network disconnect → NetworkError → PT-BR message
6. Verify all user-facing errors are in Portuguese BR
7. Check Convex function logs for proper error logging

## Implementation Order
1. Install dependencies
2. Create `llm/errors.ts`
3. Create `llm/config.ts`
4. Create `llm/models.ts`
5. Create `llm/services/TextGeneration.ts`
6. Create `llm/services/ImageGeneration.ts`
7. Create `llm/runtime.ts`
8. Create `llm/index.ts`
9. Update `enhancedPostGeneration.ts`
10. Update `regenerate.ts`
11. Fix remaining TS errors
12. Delete old `llm.ts`
13. Test end-to-end
